{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture5_homework.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEL4DU8B316pyNy6MYzKZf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lecture-series-2021/lecture5/blob/main/lecture5_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbtia1l7_CU5"
      },
      "source": [
        "# Lecture 5: Machine Learning\n",
        "- In this homework we will implement a neural network to learn to classify the type of iris plant from attributes of its flower, using the [UCI Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris).\n",
        "- Code written by Rakshit Kothari, originally from https://github.com/RSKothari/Learn-Backprop.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw-Fa74-AAdg"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ43CdfsCXqk"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import abc\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# convenience functions\n",
        "def sigmoid_(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "def softmax_(x):\n",
        "  return np.exp(x)/np.sum(np.exp(x))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A54b1DroBvP-"
      },
      "source": [
        "Class that defines our neural network model.\n",
        "Useful resources:\n",
        "- https://www.ics.uci.edu/~pjsadows/notes.pdf\n",
        "- https://www.jasonosajima.com/backprop\n",
        "- https://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k68N7zIC8F5D"
      },
      "source": [
        "[Abstract base class](https://pymotw.com/3/abc/) for a neural network (NN).\n",
        "NN layers and even other NNs can derive from this class. They will have \n",
        "to re-implement the functions marked `@abc.abstractmethod`.\n",
        "\n",
        "We will use the following terminology:\n",
        "- `x`: input to a layer or the entire NN\n",
        "- `y`: output of a layer or the entire NN\n",
        "- `p`: learnable parameters in the layer or the entire NN\n",
        "- `grad`: gradient of loss function w.r.t. `x`\n",
        "- `update`: gradient of loss function w.r.t. `p`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsZJhLNjDYuE"
      },
      "source": [
        "class Module(metaclass=abc.ABCMeta):\n",
        "  # Question: What is the purpose of this class?\n",
        "  def __init__(self):\n",
        "    # False indicates this NN is in training mode i.e. inputs will be stored for update computation later\n",
        "    # True indicates this NN is in evaluation mode\n",
        "    self.eval = True\n",
        "    self.params = []  # by default, a module will have no learnable parameters\n",
        "    self.g_params = []  # gradient of loss function w.r.t. self.params\n",
        "    self.inputs = []  # inputs stored for update computation later\n",
        "    self.g_inputs = []  # gradient of loss function w.r.t. input\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    NN forward pass i.e. compute output y from input x.\n",
        "    Also stores x in self.inputs, because it will be used in self.backward()\n",
        "    \"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def backward(self, g_outputs):\n",
        "    \"\"\"\n",
        "    NN backward pass. This function calculates:\n",
        "    - gradient of loss function w.r.t. its inputs self.inputs and stores it in self.g_inputs\n",
        "    - gradient of loss function w.r.t. its learnable parameters self.params and stores it in self.g_params\n",
        "    Inputs:\n",
        "    - g_outputs: gradient of loss function w.r.t. its output. This comes from the next layer\n",
        "    (recall the computation graph and chain rule explained in class)\n",
        "    Returns: None\n",
        "    \"\"\"\n",
        "    # If you are confused about backprop derivation,\n",
        "    # please consider going over this article:\n",
        "    # https://www.jasonosajima.com/backprop\n",
        "  \n",
        "  @abc.abstractmethod\n",
        "  def grad_fn(self, x):\n",
        "    \"\"\"\n",
        "    gradient of output w.r.t. input, for a single input\n",
        "    recall chain equation dL_din = dL_dout * dout_din\n",
        "    This function calculates dout_din\n",
        "    \"\"\"\n",
        "\n",
        "  def zero_grad(self):\n",
        "    \"\"\"\n",
        "    This function resets the NN i.e. clears out the stored inputs and gradients\n",
        "    \"\"\"\n",
        "    self.inputs = []\n",
        "    self.g_inputs = []"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABv0KG98BAuO"
      },
      "source": [
        "class ANN(Module):\n",
        "    # Question: Why do we derive ANN from Module?\n",
        "    def __init__(self, ):\n",
        "      super(ANN, self).__init__()\n",
        "      # Question: What does the \"super\" do?\n",
        "      self.operations = []\n",
        "\n",
        "    def forward(self, x):\n",
        "      \"\"\"\n",
        "      Forward neural network operation. The network stores all inputs that passed\n",
        "      through a layer. These inputs are used to compute the gradient and can be\n",
        "      freed using the zero_grad() function\n",
        "      \"\"\"\n",
        "      # Question: What does the attribute \"operations\" hold?\n",
        "      for operation in self.operations:\n",
        "        x = operation.forward(x)\n",
        "      return x\n",
        "\n",
        "    def backward(self, g_outputs):\n",
        "      \"\"\"\n",
        "      Computes the gradient for each input saved in the layer and all learnable parameters\n",
        "      \"\"\"\n",
        "      # Iterate through every operation in the network.\n",
        "      # Question: What does \"idx\" and \"operation\" hold?\n",
        "      # What is the importance of the \"enumerate\" function?\n",
        "      # set_trace()\n",
        "      for idx, operation in enumerate(reversed(self.operations)):\n",
        "        # Find the gradient of  \"operation\"\n",
        "        operation.backward(g_outputs)\n",
        "        for idx, g_input in enumerate(operation.g_inputs):\n",
        "          if operation.type == 'act_func':\n",
        "            g_outputs[idx] = g_outputs[idx] * g_input\n",
        "          else:\n",
        "            g_outputs[idx] = g_input.T.dot(g_outputs[idx])\n",
        "\n",
        "        # Question: What is the difference between doing mat_A*mat_B and\n",
        "        # mat_A.dot(mat_B)? Here, mat_A and mat_B are two matrices.\n",
        "\n",
        "    def grad_fn(self, x):\n",
        "      \"\"\"\n",
        "      We don't need dout_din for the whole ANN\n",
        "      \"\"\"\n",
        "      return None\n",
        "    \n",
        "    def zero_grad(self):\n",
        "      \"\"\"\n",
        "      A function which frees up gradients and saved input arrays\n",
        "      \"\"\"\n",
        "      for operation in self.operations:\n",
        "        operation.zero_grad()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6EhugWFCn4X"
      },
      "source": [
        "# Homework 1\n",
        "Class that implements gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izwlF-9O-pyl"
      },
      "source": [
        "class SGD():\n",
        "  \"\"\"\n",
        "  Gradient Descent class\n",
        "  (becomes stochastic when you pass minibatches to it instead of the entire dataset)\n",
        "  \"\"\"\n",
        "  def __init__(self, lr=1e-3):\n",
        "    # learning rate\n",
        "    self.lr = lr\n",
        "\n",
        "  def __call__(self, model):\n",
        "    \"\"\"\n",
        "    model: ANN object\n",
        "    \"\"\"\n",
        "    # iterate through operations in the computation graph\n",
        "    for op in model.operations:\n",
        "      # ignore a node if it is in eval mode\n",
        "      if not op.eval:\n",
        "        # apply the gradient descent update\n",
        "        for idx, dL_dP in enumerate(op.g_params):\n",
        "          # op.updates[idx] contains the gradient dL_dP of loss w.r.t.\n",
        "          # parameters op.p[idx]\n",
        "          # write the gradient descent code for updating op.p[idx] using\n",
        "          # dL_dP and self.lr\n",
        "          # op.params[idx] = "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP1pQgI6sQLI"
      },
      "source": [
        "## Solution\n",
        "Click the black arrow to the left of \"Solution\" to peek at the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqyffYYMsq4g"
      },
      "source": [
        "`op.p[idx] = op.p[idx] - self.lr*dL_dP`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eORLDNRHs2bM"
      },
      "source": [
        "# Homework 2\n",
        "Class that implements a [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) operation `y = Ax + b`.\n",
        "\n",
        "`y`, `x`, and `b` are vectors of the same size, `A` is a matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0AUtr1TDJ8Z"
      },
      "source": [
        "class linear(Module):\n",
        "  def __init__(self, in_c, out_c, bias=True):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - in_c: size of input vector\n",
        "    - out_c: size of output vector\n",
        "    \"\"\"\n",
        "    # Question: What does the __init__ function do?\n",
        "    super(linear, self).__init__()\n",
        "    self.type = 'linear'\n",
        "    self.eval = False\n",
        "    # learnable parameters of the Linear layer\n",
        "    self.params = [np.random.rand(out_c, in_c),  # Weight matrix A\n",
        "                   np.random.rand(out_c, 1)]     # Bias vector b\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Inputs:\n",
        "    - x: Input feature vector of shape [in_c, 1]\n",
        "    Outputs:\n",
        "    - y: Output feature vector y = Ax + b of shape [out_c, 1]\n",
        "    '''\n",
        "    self.inputs.append(x)\n",
        "    # y = # write your code to derive y from x and self.p\n",
        "    return y\n",
        "\n",
        "  def grad_fn(self, x):\n",
        "    \"\"\"\n",
        "    gradient of output y w.r.t. input x\n",
        "    \"\"\"\n",
        "    # dout_din = # write your code here\n",
        "    return dout_din\n",
        "\n",
        "  def backward(self, g_outputs):\n",
        "    A_grads, b_grads = [], []\n",
        "\n",
        "    # Since we pass every sample separately, we must compute the gradient\n",
        "    # for each samples separately. Loop over the stored inputs.\n",
        "    for idx, inp in enumerate(self.inputs):\n",
        "      self.g_inputs.append(self.grad_fn(inp))\n",
        "\n",
        "      # write your code for gradient of loss w.r.t. A\n",
        "      # A_grad = \n",
        "      A_grads.append(A_grad)\n",
        "\n",
        "      # write your code for gradient of loss w.r.t. b\n",
        "      # b_grad = \n",
        "      b_grads.append(b_grad)\n",
        "\n",
        "    # Add weight gradient across all samples\n",
        "    A_grad = np.sum(np.stack(A_grads, axis=0), axis=0)\n",
        "    assert A_grad.shape == self.params[0].shape, 'A grad shape does not match'\n",
        "\n",
        "    # Average bias gradient across all samples\n",
        "    b_grad = np.mean(np.stack(b_grads, axis=0), axis=0)\n",
        "    assert b_grad.shape == self.params[1].shape, 'b grad shape does not match'\n",
        "\n",
        "    self.g_params = [A_grad, b_grad]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtVUhNT5t0f-"
      },
      "source": [
        "## Solution\n",
        "Click the black arrow to the left of \"Solution\" to peek at the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbWN0Wupt52Z"
      },
      "source": [
        "- `forward()`: `y = self.params[0].dot(x) + self.params[1]`\n",
        "- `grad_fn()`: `dout_din = self.params[0]`\n",
        "- `backward()`: `A_grad = g_outputs[idx].dot(inp.T)`, `b_grad = g_outputs[idx]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcJ7iCnl4Xtr"
      },
      "source": [
        "# Homework 3\n",
        "Class that implements a [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html?highlight=sigmoid#torch.nn.Sigmoid) operation. This operation does not have any learnable parameters. It is often used to convert a vector from an arbitrary range to a `[-1, 1]` range i.e. each element of the output vector will be in `[-1, 1]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_E5DG8bDk1s"
      },
      "source": [
        "class sigmoid(Module):\n",
        "  def __init__(self, ):\n",
        "    super(sigmoid, self).__init__()\n",
        "    self.type = 'act_func'\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.inputs.append(x)\n",
        "    # write your code here for deriving output from input\n",
        "    # y = \n",
        "    return y\n",
        "\n",
        "  def grad_fn(self, x):\n",
        "    # Question: Find the derivative of sigmoid\n",
        "    # dout_din = \n",
        "    return dout_din\n",
        "\n",
        "  def backward(self, prev_grad):\n",
        "    self.g_inputs = [self.grad_fn(inp) for inp in self.inputs]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFjjjxrf48-J"
      },
      "source": [
        "## Solution\n",
        "Click the black arrow to the left of \"Solution\" to peek at the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX0gpCAx5ECc"
      },
      "source": [
        "- `forward()`: `y = sigmoid_(x)`\n",
        "- `grad_func()`: `dout_din = sigmoid_(x) * sigmoid_(1.0 - x)`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV8nbcChB50o"
      },
      "source": [
        "# Homework 4\n",
        "Class the implements [MSE Loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC6WNDj6DibG"
      },
      "source": [
        "class MSE_loss():\n",
        "  def __init__(self):\n",
        "    self.type = 'loss_func'\n",
        "\n",
        "  def forward(self, y_pred, y_gt):\n",
        "    \"\"\"\n",
        "    Inputs (both same shape):\n",
        "    - y_pred: prediction vector\n",
        "    - y_gt: ground truth vector\n",
        "    Returns\n",
        "    Loss value (scalar)\n",
        "    \"\"\"\n",
        "    # write your code here for MSE loss\n",
        "    # loss = \n",
        "    return loss\n",
        "\n",
        "  def grad_fn(self, y_pred, y_gt):\n",
        "    \"\"\"\n",
        "    Gradient of loss w.r.t. y_pred\n",
        "    \"\"\"\n",
        "    # Question: Find the gradient of MSE loss w.r.t. y_pred\n",
        "    # dout_din = \n",
        "    return dout_din\n",
        "\n",
        "  def __call__(self, y_pred, y_gt):\n",
        "    # Question: What does __call__ do?\n",
        "    return self.forward(y_pred, y_gt), self.grad_fn(y_pred, y_gt)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyOXBZFrCLZP"
      },
      "source": [
        "## Solution\n",
        "Click the black arrow to the left of \"Solution\" to peek at the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhgMzg4ODVOz"
      },
      "source": [
        "- `forward()`: `loss = 0.5*np.mean(np.power(y_pred - y_gt, 2))`\n",
        "- `grad_fn()`: `dout_din = y_pred - y_gt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkmkgqUeCNbK"
      },
      "source": [
        "# Homework 5\n",
        "Class the implements the [Cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogMpIQncDfLW"
      },
      "source": [
        "class CrossEntropyLoss():\n",
        "  def __init__(self, n_classes):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - n_classes = Total number of classes\n",
        "    \"\"\"\n",
        "    self.type = 'loss_func'\n",
        "    self.n_classes = n_classes\n",
        "\n",
        "  def forward(self, y_pred, y_gt):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - y_pred: vector of prediction \"logits\", shape [self.n_classes, 1]\n",
        "    - y_gt: ground truth class label, scalar int in [0, self.n_classes]\n",
        "    Returns:\n",
        "    - scalar loss value\n",
        "    \"\"\"\n",
        "    # write your code for cross entropy loss\n",
        "    # loss = \n",
        "    return loss\n",
        "\n",
        "  def grad_fn(self, y_pred, y_gt):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - y_pred: vector of prediction \"logits\", shape [self.n_classes, 1]\n",
        "    - y_gt: ground truth class label, scalar int in [0, self.n_classes]\n",
        "    Returns:\n",
        "    - dout_din: Gradient of loss w.r.t. y_pred, vector of shape [self.n_classes, 1]\n",
        "    \"\"\"\n",
        "    # write your code for gradient of loss w.r.t. y_pred\n",
        "    # dout_din = \n",
        "    return dout_din\n",
        "\n",
        "  def __call__(self, y_pred, y_gt):\n",
        "    return self.forward(y_pred, y_gt), self.grad_fn(y_pred, y_gt)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEJ6gSpMCWE4"
      },
      "source": [
        "## Solution\n",
        "Click the black arrow to the left of \"Solution\" to peek at the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqyJdGUGGikM"
      },
      "source": [
        "- `forward()`: `loss = -np.log(softmax_(y_pred))[y_gt]`\n",
        "- `grad_fn()`: `dout_din = softmax_(y_pred) - 1*(np.arange(self.n_classes)==y_gt)[:, np.newaxis]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvz4VU84D_O9"
      },
      "source": [
        "# Training\n",
        "You just have to run this cell. If your answers above were correct, you should get > 97% accuracy.\n",
        "You can also try changing the various hyperparameters -- size of linear layer, learning rate, initialization strategy, etc. to improve beyond that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5KeBppoEBaM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "c37c15c8-57ea-429c-f1ec-2f85572c9575"
      },
      "source": [
        "lr = 1e-3\n",
        "steps = 1000\n",
        "\n",
        "iris_data = load_iris()\n",
        "feats = iris_data['data']\n",
        "gt = iris_data['target']\n",
        "\n",
        "num_samples = feats.shape[0]\n",
        "num_classes = gt.max() + 1\n",
        "\n",
        "net = ANN()\n",
        "\n",
        "net.operations = [linear(in_c=4, out_c=64),\n",
        "                  sigmoid(),\n",
        "                  linear(in_c=64, out_c=num_classes)]\n",
        "\n",
        "loss_func = CrossEntropyLoss(num_classes)\n",
        "\n",
        "optimize = SGD(lr=lr)\n",
        "\n",
        "samples = np.split(feats, num_samples, axis=0)\n",
        "targets = np.split(gt, num_samples, axis=0)\n",
        "predict = np.random.choice(num_classes, size=num_samples)  # start with random predictions\n",
        "losses = []\n",
        "\n",
        "for step in range(steps):\n",
        "  if (step > 0) and (step % 100 == 0):\n",
        "    print(f'Step {step+1}/{steps}, Loss = {np.mean(losses):.4f}, Accuracy = {100.0*np.mean(predict==gt):.2f}%')\n",
        "  loss_per_epoch = 0\n",
        "  g_losses = []\n",
        "  \n",
        "  for idx, sample in enumerate(samples):\n",
        "    out = net.forward(sample.T)  # Read note above\n",
        "    loss, g_loss = loss_func(out, targets[idx])\n",
        "    g_losses.append(g_loss)\n",
        "    loss_per_epoch += loss.squeeze()\n",
        "    predict[idx] = np.argmax(out)\n",
        "\n",
        "  loss_per_epoch = loss_per_epoch/len(samples)\n",
        "  losses.append(loss_per_epoch)\n",
        "  # print(f'Loss = {loss_per_epoch:.3f}')\n",
        "  # Generate gradients to update weights ...\n",
        "  net.backward(g_losses)\n",
        "  # Gradients have been generated, time to update weights!\n",
        "  optimize(net)\n",
        "  net.zero_grad()\n",
        "\n",
        "c_mat = confusion_matrix(gt, predict)\n",
        "c_mat = c_mat.astype('float') / c_mat.sum(axis=1)[:, np.newaxis]\n",
        "plt.plot(losses)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 101/1000, Loss = 1.2256, Accuracy = 66.67%\n",
            "Step 201/1000, Loss = 1.0937, Accuracy = 66.67%\n",
            "Step 301/1000, Loss = 1.0253, Accuracy = 66.67%\n",
            "Step 401/1000, Loss = 0.9672, Accuracy = 67.33%\n",
            "Step 501/1000, Loss = 0.8844, Accuracy = 88.67%\n",
            "Step 601/1000, Loss = 0.7608, Accuracy = 97.33%\n",
            "Step 701/1000, Loss = 0.6691, Accuracy = 97.33%\n",
            "Step 801/1000, Loss = 0.5988, Accuracy = 97.33%\n",
            "Step 901/1000, Loss = 0.5433, Accuracy = 97.33%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1cacf366d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "0F3BsTyBE_4V",
        "outputId": "3159ebdd-1455-4c00-d4dd-9faf0313ae11"
      },
      "source": [
        "plt.imshow(c_mat)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1caceefed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}